{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3knkHZcWKp2k"
      },
      "source": [
        "# Project: Disaster Relief\n",
        "#Section 2: Data Preprocessing and Simple Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5gBwlTa5EUR"
      },
      "source": [
        "Today we will be building some more sophisticated models called One-hot-encoding, CountVectorizer, and Bag of Words. You can review the slides to remember how these models work. But first we will need to clean our data a little bit, an important step in any AI Pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbW0_pxGhSKM"
      },
      "source": [
        "In this notebook we'll be:\n",
        "*   Preprocessing the Dataset for ML\n",
        "*   Implementing a Bag of Words Vectorizer\n",
        "*   Implementing Logistic Regression ML Models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpVg1HHxHoV9",
        "outputId": "0db22f9e-9c43-4f4e-af37-8fc5e6a2470a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\rdisaster_data.csv     0%[                    ]       0  --.-KB/s               \rdisaster_data.csv   100%[===================>] 222.42K  --.-KB/s    in 0.003s  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Load your dataset { display-mode: \"form\" }\n",
        "# Run this every time you open the spreadsheet\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from collections import Counter\n",
        "from random import sample\n",
        "from importlib.machinery import SourceFileLoader\n",
        "import numpy as np\n",
        "from os.path import join\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import seaborn as sns\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords' ,quiet=True)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Disaster%20Relief/disaster_data.csv'\n",
        "dataset_path = './disaster_data.csv'\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "ACwiMJ0uC7Ca",
        "outputId": "0355afdd-1e9e-426f-8a9b-91e2f0324cb4"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchtext'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6fd42c9d6d17>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGloVe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#@title If the previous cell fails to load data, use this cell\n",
        "import re\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchtext.vocab import GloVe\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import requests, io, zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYFUn48ZeX5C"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHy-dPU0c7bD"
      },
      "source": [
        "Up until now, we have looked at disaster-related tweets, made a rule-based classifier, and evaluated it. Today, we shall clean the data, stem it, and learn about One-Hot Encoding, CountVectorizer, and Logistic Regression.\n",
        "\n",
        "Before, we do all this, we must understand what a model really is."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyDHWJg1eZdG"
      },
      "source": [
        "What is a model? A model is something that we will make to predict the category of a given tweet. It is a numerical understanding of the data such that given a new data point, we can figure out how it links to the previous data. This is a crude definition, which you will understand more through this project.\n",
        "\n",
        "The models we have used or will use in this project are:\n",
        "\n",
        "1. Rule Based Classifier (You specify rules based on which the model gives the output - the category of the tweet)\n",
        "2. CountVectorizer + Logistic Regression: A model based on counting the number of occurances of a word and applying regression on it to predict the category of a new tweet\n",
        "3. Word2Vec + Logistic Regression: A model which applies the idea that words that occur in similar contexts tend to be close in a sentence, and uses this to predict the category of the tweet.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCUv39e9UbZS"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Before we can jump into building a model, we must clean the data a little bit!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1xsl3tDctOMo"
      },
      "outputs": [],
      "source": [
        "# Load the data.\n",
        "disaster_tweets = pd.read_csv('disaster_data.csv',encoding =\"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOINtlLace03"
      },
      "source": [
        "**Exercise:** Show the top five tweets in our `disaster_tweets` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YBMFak2mdcf0"
      },
      "outputs": [],
      "source": [
        "### Your code here\n",
        "disaster_tweets.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6YI_yKLtOMu"
      },
      "source": [
        "**Discussion Exercise**: Consider the tweet *I really need food.... I am very hungry. The hunger is unbearable. #pleasehelp*\n",
        "\n",
        "1. Are all words in the tweet equally informative?\n",
        "2. Are there any words in this sentence that mean the same thing, but are technically distinct words?\n",
        "3. Are there any unnecessary words or symbols that we could remove from the tweet before building a model?\n",
        "\n",
        "We are going to play with three pre-processing steps to address these two questions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA464Y6cLTvY"
      },
      "source": [
        "###Removal of non alphabetic characters\n",
        "\n",
        "In tweet classification we use words as the features, so it's important to remove unwanted characters such as numbers and punctuation marks as they don't provide us with any valuable information.\n",
        "\n",
        "To do this, we'll use **regular expressions**. Regular expressions are a powerful tool that perform *pattern matching*, allowing us to search a piece of text for patterns and replace all occurrences of a pattern with something else.\n",
        "\n",
        "For example, suppose we want to remove all occurrences of the letter \"X\" from a block of text (for whatever reason). This is the same as replacing the letter \"X\" with the empty string \"\". We can use the function `re.sub` to do this in Python:\n",
        "\n",
        "> `text = re.sub(r\"X\", \"\", text)`\n",
        "\n",
        "Below, we use `tweets = tweets.apply(lambda x: re.sub(r'[^a-zA-Z0-9]+', ' ',x))` to use regular expressions on every tweet in the `tweets` data.\n",
        "\n",
        "What is our `re.sub` function replacing?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ipcc-gW4pV_H"
      },
      "outputs": [],
      "source": [
        "#Read the tweet data and convert it to lowercase\n",
        "tweets = disaster_tweets['text'].str.lower()\n",
        "tweets = tweets.apply(lambda x: re.sub(r'[^a-zA-Z0-9]+', ' ',x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZZ4U8NqwLqkC"
      },
      "outputs": [],
      "source": [
        "#Extract the labels from the csv\n",
        "tweet_labels = disaster_tweets['category']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZegZdHEtOMu"
      },
      "source": [
        "### Tokenizing\n",
        "First we need to split a sentence into individual words, or *tokens*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBHJd1zzbP5y"
      },
      "source": [
        "###**Discussion Exercise**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhQawpZhZh0i"
      },
      "source": [
        "What would be the token list be of the following sentence be? \"*AI is so fun! I love to learn about NLP and machine learning!*\"\"\n",
        "\n",
        "Discuss your answer with your group, then write it down in your worksheet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "d9Kp7TmUP4jV"
      },
      "outputs": [],
      "source": [
        "#@title Tokenizing\n",
        "tweet = \"AI is so fun! I love to learn about NLP and machine learning! Enter your own tweet here\" #@param {type:'string'}\n",
        "for i in word_tokenize(tweet):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOdgIHG1XyTc"
      },
      "source": [
        "## Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxy-DT6VX1ow"
      },
      "source": [
        "Remember that the goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
        "\n",
        "A difference between stemming and lemmatization is that stemming looks at the current word only, while lemmatization also takes the context into consideration. Either way, this pre-processing step could be somewhat tedious. Luckily, the powerful `nltk`, (short for the [National Language Toolkit](https://www.nltk.org/) library), provides tools for both.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqiJUBS_tONG"
      },
      "source": [
        "\n",
        "### Exercise: Stemming using the Porter stemmer\n",
        "*Porter's algorithm*, developed in the 1980s, is one of the most commonly used stemmers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOZgX-PUX6c8"
      },
      "source": [
        "Try and find a word that Porter's stemming doesn't work well on! (Hint: Try some plurals of words that end in -e)\n",
        "\n",
        "What happens when you input multiple plural words?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LZJvHmIttONI"
      },
      "outputs": [],
      "source": [
        "#@title Stem words { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "stemmer = PorterStemmer()\n",
        "word = \"leaves\" #@param {type:\"string\"}\n",
        "print(stemmer.stem(word))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDL4h--FtONM"
      },
      "source": [
        "### Lemmatizer\n",
        "\n",
        "With the stemmer, you might have found that the results may look a bit mechanical. This is because the Porter's algorithm is essentially a sequential application of a set of rules. To get better looking results, let's try out a lemmatizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VidwNdk0tONO"
      },
      "outputs": [],
      "source": [
        "#@title Lemmatize Words { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "# Get the lemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "lemma = WordNetLemmatizer()\n",
        "word = \"leaves\" #@param {type:\"string\"}\n",
        "print(lemma.lemmatize(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fRexsQWZJCO"
      },
      "source": [
        "###**Discussion Exercise**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lfFIoeOJ-mS"
      },
      "source": [
        "**Discuss:**\n",
        "* What are the differences between the Porter stemmer and the lemmatizer?\n",
        "* How do you think the lemmatizer works?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRcZX84-x06q"
      },
      "source": [
        "## Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IMUAPkRbasd"
      },
      "source": [
        "###**Discussion Exercise**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "aPwRMEkvl-ZA"
      },
      "outputs": [],
      "source": [
        "#@title Are there words that can be removed without affecting the model?\n",
        "\n",
        "#@markdown Hint: Which words can we remove without changing the meaning of a sentence?\n",
        "words = \"so\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yF6R42-zoUp"
      },
      "source": [
        "Stop words are words that occur in both categories, that are not relevant to the context, such as 'at', 'is', 'the' and so on... It is usually advantageous for the classifier to ignore these stop words, since they may add noises or cause numerical issues as they add baggage to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RIsSdBJ14Zn_"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell for some example stop words! { vertical-output: true, display-mode: \"form\" }\n",
        "#@markdown *Your groupmates will likely have different examples.*\n",
        "eng_stopwords = stopwords.words('english')\n",
        "for i, word in enumerate(sample(eng_stopwords, 10)):\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBPkmmzF4mWU"
      },
      "source": [
        "Let us see if the words you identified are stop words or not. Input them down below to see!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gw75nnP14t-k"
      },
      "outputs": [],
      "source": [
        "#@title Check stop words { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "word = \"inside\" #@param {type:\"string\"}\n",
        "if not word: raise Exception('Please enter a word')\n",
        "eng_stopwords = set(stopwords.words('english'))\n",
        "if word.lower().strip() in eng_stopwords: print('YES')\n",
        "else: print('NO')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fK0FSbhRA2r"
      },
      "source": [
        "## Preprocessing pipeline of our data\n",
        "\n",
        "Explore how combining these methods changes the structure of our tweet dataset. Change out the `None` values to complete our `remove_stopwords()` function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ULJG6XbIpxsZ"
      },
      "outputs": [],
      "source": [
        "stopword_set = set(stopwords.words('english'))\n",
        "\n",
        "# Complete the following function to remove the stopwords from the tokenized tweets\n",
        "def remove_stopwords(token_list):\n",
        "  filtered_sentences = []\n",
        "  ### YOUR CODE HERE\n",
        "  for tweet in token_list:\n",
        "    new_tweet = []\n",
        "    for word in tweet:\n",
        "      if word not in stopword_set:\n",
        "        new_tweet.append(word)\n",
        "    filtered_sentences.append(new_tweet)\n",
        "  ### END CODE\n",
        "  return filtered_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG6ZXScigzu1"
      },
      "source": [
        "Remember the cell we ran at the top of this notebook to import our data? Inside, we imported the helper function `word_tokenize` from `nltk`. Below, we use `word_tokenize` to tokenize every tweet in our `tweets` data.\n",
        "\n",
        "**Exercise:** Use the `remove_stopwords()` function we defined above to remove the stopwords in our `tokenized_tweets` data and print the first five!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ho4sPpFJRFy2"
      },
      "outputs": [],
      "source": [
        "# Tokenize all the tweets\n",
        "tokenized_tweets = [word_tokenize(t) for t in tweets]\n",
        "\n",
        "### YOUR CODE HERE\n",
        "tweet_set = remove_stopwords(tokenized_tweets)\n",
        "# Remove stopwords from all the tweets\n",
        "for i in range(5):\n",
        "  print(\"Original tweet: %s: \\nCleaned and tokenized data: %s\\n\" % (tweets[i], tweet_set[i]))\n",
        "\n",
        "### END CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYmIUGamOuaN"
      },
      "source": [
        "\n",
        "# Bag of Words Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_fBuMp_UsO8"
      },
      "source": [
        "## One-Hot Encoding\n",
        "**One-Hot Encoding**, also known as one-of-K scheme is a way to encode the data to be used in other functions (such as linear regression).\n",
        "\n",
        "Let us consider an example to understand one-hot encoding:\n",
        "\n",
        "Before we apply a model on our tweets, we need to convert it to a form the model, i.e. a machine, can understand - essentially convert a tweet to numerical form. We cannot just pass words to the model, because it won't know what those mean and might try and extract information from them. Hence, a numerical format is the best.\n",
        "\n",
        "The easiest way to do so is to map each word in a tweet to a number, a categorical value. This will represent all words in a tweet uniquely!\n",
        "\n",
        "---\n",
        "Suppose we have a tweet:\n",
        "> Tweet: 'I am hungry need food' <br>\n",
        "> Category: Food\n",
        "\n",
        "Its numerical representation would be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kqSH4SwDUrol"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to see the numeric representation! { vertical-output: true, display-mode: \"form\" }\n",
        "d = {'I': 1, 'am': 2, 'hungry': 3, 'need': 4, 'food': 5}\n",
        "print('{:<12}|{:>2}'.format('word', 'value'))\n",
        "print('-------------------')\n",
        "for k,v in d.items(): print('{:<12}|{:>3}'.format(k,v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b-n3laDc1ZS"
      },
      "source": [
        "**Discussion Exercise**: Why do you think the above representation is wrong? What information could a model possibly extract from the above information such that its conclusions would be wrong/way off to what we want to achieve?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6M5gJ9dScQ"
      },
      "source": [
        "We need to encode the words and include them as a feature to train the model. That is where One-Hot Encoding comes into play. Understanding how it is done will make the process clearer.\n",
        "\n",
        "Let us consider the same example and see what its one-hot encoding would be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xzqRYQVfKYwZ"
      },
      "outputs": [],
      "source": [
        "#@title Run this cell to see the One-Hot Encoding! { vertical-output: true, display-mode: \"form\" }\n",
        "print('{:^10}|{:^7}|{:^8}|{:^2}|{:^9}'.format('I', 'am', 'hungry','need','food'))\n",
        "print('---------------------------------------------------')\n",
        "print('{:^10}|{:^7}|{:^8}|{:^4}|{:^9}'.format('1', '0', '0','0','0'))\n",
        "print('{:^10}|{:^7}|{:^8}|{:^4}|{:^9}'.format('0', '1', '0','0','0'))\n",
        "print('{:^10}|{:^7}|{:^8}|{:^4}|{:^9}'.format('0', '0', '1','0','0'))\n",
        "print('{:^10}|{:^7}|{:^8}|{:^4}|{:^9}'.format('0', '0', '0','1','0'))\n",
        "print('{:^10}|{:^7}|{:^8}|{:^4}|{:^9}'.format('0', '0', '0','0','1'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4ihznS8vA3p"
      },
      "source": [
        "Above, each letter is represented using a row of 1's and 0's, a row which can essentially represent the whole of the vocabulary. This is called one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "8OBn9xhCU4lF"
      },
      "outputs": [],
      "source": [
        "#@title (Optional): What are the encodings for the inputs below?\n",
        "i = \"[1, 0, 0, 0, 0]\" #@param [\"Choose an answer\", \"[1, 0, 0, 0, 0]\", \"[1, 1, 0, 0, 1]\", \"[1, 0, 0, 1, 1]\", \"[1, 0, 1, 0, 0]\", \"[0, 1, 0, 1, 1]\"]\n",
        "i_hungry = \"[1, 0, 1, 0, 0]\" #@param [\"Choose an answer\", \"[1, 0, 0, 0, 0]\", \"[1, 1, 0, 0, 1]\", \"[1, 0, 0, 1, 1]\", \"[1, 0, 1, 0, 0]\", \"[0, 1, 0, 1, 1]\"]\n",
        "i_need_food = \"[1, 0, 0, 1, 1]\" #@param [\"Choose an answer\", \"[1, 0, 0, 0, 0]\", \"[1, 1, 0, 0, 1]\", \"[1, 0, 0, 1, 1]\", \"[1, 0, 1, 0, 0]\", \"[0, 1, 0, 1, 1]\"]\n",
        "\n",
        "answers = [i, i_hungry, i_need_food]\n",
        "words = [\"i\", \"i_hungry\", \"i_need_food\"]\n",
        "encodings = [\"[1, 0, 0, 0, 0]\",\n",
        "           \"[1, 0, 1, 0, 0]\",\n",
        "           \"[1, 0, 0, 1, 1]\"]\n",
        "\n",
        "for i, actual in enumerate(encodings):\n",
        "  if actual == answers[i]:\n",
        "    print(\"Yes, that's the encoding for '{}'!\".format(words[i]))\n",
        "  else:\n",
        "    print(\"No, that's not the encoding for '{}'.\".format(words[i]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYf3ejtQ92Jz"
      },
      "source": [
        "## **Terms to Know:**\n",
        "\n",
        "Before we code our One-Hot Encoding function, let's get to know some new data structures we'll be seeing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSLe66OS94XV"
      },
      "source": [
        "### Sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBXd1h1g96FZ"
      },
      "source": [
        "A set is an unordered collection of unique elements.\n",
        "\n",
        "You can create sets like this:\n",
        "```\n",
        "integer_set = {5, 6, 7, 8}\n",
        "string_set = {\"hello\", \"world\"}\n",
        "```\n",
        "You can also turn a list into a set using `set()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SUaxoTAU98Nq"
      },
      "outputs": [],
      "source": [
        "my_list = [1, 1, 2, 3, 4]\n",
        "list_to_set = set(my_list)\n",
        "\n",
        "print(list_to_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqC5Ll34Aav8"
      },
      "source": [
        "Notice that `my_list` had two 1's, and `list_to_set` only has one. Sets do not contain duplicate elements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbcR9eCx9-lP"
      },
      "source": [
        "### Dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUgOdS0B-B0_"
      },
      "source": [
        "A dictionary is an unordered data structure containing key-value pairs, where each key maps to its value.\n",
        "\n",
        "You can create dictionaries like this:\n",
        "```\n",
        "grocery_dict = {1: \"bread\", 2: \"bananas\", 3: \"milk\"}\n",
        "pet_dict = {\"cats\": 5, \"dogs\": 3, \"rabbits\": 2}\n",
        "```\n",
        "Let's use `pet_dict` as an example. The key `cats` points to the value `5`. Run the cell below to see how we can access a value from a dictionary.\n",
        "\n",
        "Try to access other values!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b5ttg15T-H3i"
      },
      "outputs": [],
      "source": [
        "pet_dict = {\"cats\": 5, \"dogs\": 3, \"rabbits\": 2}\n",
        "\n",
        "num_cats = pet_dict[\"rabbits\"] # Change this!\n",
        "print(num_cats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBe388r1-Ld9"
      },
      "source": [
        "We can also change the value in a key-value pair:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "i2BLUhgC-OeT"
      },
      "outputs": [],
      "source": [
        "pet_dict[\"cats\"] = 0 # Change this!\n",
        "\n",
        "print(pet_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDZwtd6L-bnG"
      },
      "source": [
        "Here's how we might check if a key is in a dictionary and add it if not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wpqfnrL7-dln"
      },
      "outputs": [],
      "source": [
        "pet = \"lizard\" # Change this!\n",
        "\n",
        "if pet in pet_dict:\n",
        "  print(\"Yes, our pet_dict contains one or more\", pet)\n",
        "else:\n",
        "  pet_dict[pet] = 1 # Change this!\n",
        "\n",
        "print(pet_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ATl2Wn-hPO"
      },
      "source": [
        "Feel free to revisit the above section when needed. Sets and dictionaries are useful data structures!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqy3nTWDVYrt"
      },
      "source": [
        "### One-Hot Encoding Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p6YCzS_zyIl"
      },
      "source": [
        "**Exercise:** Complete the `one_hot_encoding` function below!\n",
        "\n",
        "**Note:** You'll use the function `enumerate(list)`. We can use `enumerate()` to keep track of elements of a list passed in, and their respective indices `idx`. Read more about `enumerate()` [here](https://realpython.com/python-enumerate/)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "colab": {
          "background_save": true
        },
        "id": "NoMBPD60VYMd"
      },
      "outputs": [],
      "source": [
        "def one_hot_encoding(sentences, sentence, print_word_dict = False):\n",
        "  \"\"\"\n",
        "  param: sentences - list of sentences to form the one-hot encoding\n",
        "  param: sentence - sentence to return the one-hot encoding of\n",
        "  return: sent_encoding - the encoded sentence\n",
        "  \"\"\"\n",
        "\n",
        "  words_list = [] # an empty list\n",
        "\n",
        "  ### YOUR CODE HERE: Change the None values!\n",
        "\n",
        "  # Step 1: Tokenize each sentence using word_tokenize() and add it to the list\n",
        "  for sent in sentences:\n",
        "    words_list.extend(word_tokenize(sent))\n",
        "\n",
        "  # Step 2: Remove the duplicates in your words_list\n",
        "  words_list = set(words_list) # Hint: See Sets above!\n",
        "\n",
        "  # Step 3: Make an empty dictionary\n",
        "  words_map_dict = {}\n",
        "\n",
        "  # Step 4:\n",
        "  # Add every word in your words_list as a key to your dictionary\n",
        "  # Pair each word with its index as the value\n",
        "  for idx, w in enumerate(words_list):\n",
        "    words_map_dict[w] = idx\n",
        "\n",
        "  # a numpy array of all zeros of the same length as words_list\n",
        "  sent_encoding = np.zeros(len(words_list)) # Our future one-hot encoding!\n",
        "\n",
        "  # Step 5:\n",
        "  # Loop through the words in the given sentence (sentence to check)\n",
        "  # Find the index of each word using our dictionary\n",
        "  # Increment the value of the numpy array at that index by one\n",
        "  for word in word_tokenize(sentence):\n",
        "    if word in words_map_dict:\n",
        "      print(word)\n",
        "      idx = words_map_dict[word]\n",
        "      sent_encoding[idx] += 1\n",
        "\n",
        "  ### END CODE\n",
        "\n",
        "  if print_word_dict:\n",
        "    print (\"Word Dictionary: \", words_map_dict)\n",
        "\n",
        "  return sent_encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n3rJZr_hqsUX"
      },
      "outputs": [],
      "source": [
        "encoding = one_hot_encoding([\"oh I love AI\", \"AI is so much fun\", \"is AI fun or what\"], \"oh what fun AI is \", True)\n",
        "print(\"Encoding: \", encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGIjELh0wDD1"
      },
      "source": [
        "## Count Vectorizer - The Bag of Words Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uZDNaLHdxbl"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi7A3SuRwGdP"
      },
      "source": [
        "Now that we have the one-hot encoding for all tweets, it's time to attach value to each word, such that it can be distinguished and used by the model.\n",
        "\n",
        "One idea is to assign each word the same weight, say a weight of one. However, that is non distinguishable and the model won't be able to learn anything with that information. Can you think of a simple way to add weights to words which, say, occur more frequently?\n",
        "\n",
        "That is the Bag of Words Model. The **Bag of Words Model** converts the tweets into a matrix of token counts. Let us consider an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebbwyut2jxgl"
      },
      "source": [
        "Recall the first 3 tweets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bznJPPrHeAA5"
      },
      "outputs": [],
      "source": [
        "for t in tweets[:3]:\n",
        "  print(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_nJ-Kqtkvv6"
      },
      "source": [
        "The Vocabulary for the tweets would be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Ax8Y4gQCsibD"
      },
      "outputs": [],
      "source": [
        "#@title Vocabulary { vertical-output: true, display-mode: \"form\" }\n",
        "word_count = Counter()\n",
        "for tweet in tweets[:3]:\n",
        "  for t in word_tokenize(tweet):\n",
        "    word_count[t]+=1\n",
        "word_count_list = [(k,v) for k,v in word_count.items()]\n",
        "word_count_list.sort(key=lambda x:x[0])\n",
        "print('{:<12}|{:>2}'.format('word', 'position'))\n",
        "print('-------------------')\n",
        "for k,v in enumerate(word_count_list): print('{:<12}|{:>3}'.format(v[0],k))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkY5OPdZ1XXR"
      },
      "source": [
        "If there are N words in the vocabulary, each row in the matrix would be of N words.\n",
        "\n",
        "Based on this, one-hot encoding for each tweet and the matrix that will be given to the model would be the following.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JHKB2Kt3egP6"
      },
      "outputs": [],
      "source": [
        "print(\"Tweet 1: \", one_hot_encoding([str(t) for t in tweets[:3]], str(tweets[0])))\n",
        "print(\"Tweet 2: \", one_hot_encoding([str(t) for t in tweets[:3]], str(tweets[1])))\n",
        "print(\"Tweet 3: \", one_hot_encoding([str(t) for t in tweets[:3]], str(tweets[2])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OwwUjET5z_NY"
      },
      "outputs": [],
      "source": [
        "#@title Token Counts { vertical-output: true }\n",
        "print('{:<12}|{:>2}'.format('word', 'word_count'))\n",
        "print('-------------------')\n",
        "for k,v in word_count_list: print('{:<12}|{:>3}'.format(k,v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leEZ2wKv4Iie"
      },
      "source": [
        "The above is the Bag of Words model. Sklearn's `Countvectorizer` does the same thing, however, in a much more sophisticated manner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V-TMn3LZ4g5N"
      },
      "outputs": [],
      "source": [
        "train_text = [t for t in tweets[:3]]\n",
        "print(train_text)\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_text)\n",
        "print('Number of Words in Vocabulary of train tweets are: {}'.format(len(vectorizer.vocabulary_)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0J4tRe1UrL3"
      },
      "source": [
        "And the vocabulary is *almost* the same as ours above.\n",
        "\n",
        "**Discussion Exercise**: What is different about vocabulary results from the `CountVectorizer` than our method above?   Why do you think that is? Which is better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bzb-T1z6UnZI"
      },
      "outputs": [],
      "source": [
        "vectorizer.vocabulary_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg5gkx3jI9CG"
      },
      "source": [
        "### CountVectorizer - Fit and Transform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HcyuzbvJCN6"
      },
      "source": [
        "The `.fit()` method fits the given data to the vectorizer - consider it as if it is learning from the data. It produces a matrix, which will then be passed on to the logistic regression (later, so don't worry about understanding that). Think of the `.fit()` method this way: it just allows you to generate the matrix (but doesn't let us see it) used by the model to learn information about the training data.\n",
        "\n",
        "The `.transform()` method transforms the given data to the matrix- there is no learning here. It does not generate a vocabulary, nor does it allow for many other functionalities that the `.fit()` method allows for. Hence, it cannot be used as a substitute for the `.fit()` method as it just spews out the matrix representation of the data!<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lawl8vijIMIs"
      },
      "source": [
        "### Coding Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRlc2lohePfX"
      },
      "source": [
        "Let us make the `CountVectorizer` and use the `.fit()` and `.transform()` methods to learn their workings. Fill out the code below and try it on tweets of your choice!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "895LTm1pKjpF"
      },
      "outputs": [],
      "source": [
        "tweet_01 = 'This is a big tweet '\n",
        "tweet_02 = 'Sample Tweet two'\n",
        "# You can add more tweets here\n",
        "train_text = [tweet_01, tweet_02]\n",
        "print(train_text)\n",
        "\n",
        "tweet_03 = 'This This sample tweet' # Transform this once the vectorizer is ready\n",
        "\n",
        "### Your code starts here ###\n",
        "\n",
        "# Initialize the vectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "# Fit the train_text data to the vectorizer\n",
        "vectorizer.fit(train_text)\n",
        "# Print the vocabulary of the vectorizer\n",
        "print(vectorizer.vocabulary_)\n",
        "# Transform this tweet_03's tokenList\n",
        "print(vectorizer.transform([tweet_03]))\n",
        "\n",
        "### Your code ends here ###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahjK7AgNZUx6"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS_FxE83RBuU"
      },
      "source": [
        "## Logistic Regression Refresher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAd_hJNd0Ul5"
      },
      "source": [
        "We've just spent the last week or so learning about more sophisticated neural network architectures.  Remember that logistic regression is just linear regression followed by a sigmoid function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zmV-2E-VCdL"
      },
      "source": [
        "**Review:** What is Logistic Regression?\n",
        "\n",
        "Logistic regression is a type of linear regression that is generally used for classification. Unlike linear regression which outputs continuous number values, logistic regression uses the logistic function, also called the sigmoid function, to transform the output to return a probability value between 1 and 0, which can then be mapped to the different categories. The logistic (sigmoid) function looks something like this:\n",
        "\n",
        "![Logistic Function](https://ml-cheatsheet.readthedocs.io/en/latest/_images/sigmoid.png)\n",
        "\n",
        "Consider an example to understand logistic regression and to enhance the difference between logistic and linear regression:\n",
        "\n",
        "Given data on time spent studying and exam scores:\n",
        "\n",
        ">**Linear Regression** could help us predict the student’s test score on a scale of 0 - 100. Linear regression predictions are continuous (numbers in a range).\n",
        "\n",
        ">**Logistic Regression** could help us predict whether the student passed or failed. Logistic regression predictions are discrete (only specific values or categories are allowed). We can also view probability scores underlying the model’s classifications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlicWtTrRHPC"
      },
      "source": [
        " **Why Logistic Regression?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue2m3durVBE2"
      },
      "source": [
        "**A couple of reasons for using Logistic Regression:**\n",
        "1.   Using a simpler model tells us how much room we have to improve.\n",
        "2.   A simple model makes iteration quick and easy.\n",
        "3.   Lastly, and perhaps most importantly, logistic regression is interpretable. You may have heard in the past that one thing deep neural networks struggle with is interpretability–when you are using these models to make predictions that affect people's wellbeing (e.g., sentencing decisions, predictive policing decisions), it becomes extremely important that you are able to understand why a model is making the predictions it makes.For simpler models like logistic regression, we get interpretability for free!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwJUL4QDQ-KB"
      },
      "source": [
        "## Logistic Regression in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NluqLKKQ8SNv"
      },
      "source": [
        "Logistic regression in python can be done easily with the help of sklearn's `LogisticRegression()` function. Let us first do it for the three tweet examples that we saw above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I0xxnn1i8KoK"
      },
      "outputs": [],
      "source": [
        "tweet1 = 'please help we desperately need food'\n",
        "label1 = \"Food\"\n",
        "tweet2 = 'We are very thirsty please send water'\n",
        "label2 = 'Water'\n",
        "tweet3 = 'we need water and are very thirsty'\n",
        "label3 = 'Water'\n",
        "\n",
        "train_tweets = [tweet1, tweet2]\n",
        "train_tweets_label = [label1, label2]\n",
        "test_tweets = [tweet3]\n",
        "test_tweets_label = [label3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVEdW2d8RO2i"
      },
      "source": [
        "Next, let us make the vectorizer and encode tweets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_aHfIV639W-o"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "train_vect = vectorizer.fit_transform(train_tweets) # fit_transform fits the train tweets and returns the sparse matrix of the tweets\n",
        "model = LogisticRegression()\n",
        "model.fit(train_vect, train_tweets_label) # Fit the data values to the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAzZHe6F-iOI"
      },
      "source": [
        "Over here, `model.fit(train_vect, train_tweets_label)` applies logistic regression to the data given by the  matrix and hence, fits the data to the function. It takes two arguments: the matrix that is the `train_vect` variable and the `train_tweets_label`, which is the category each tweet belongs to.\n",
        "\n",
        "Below, we transform the `test_tweets` into a vector form. Now predict the third tweet using this model.\n",
        "\n",
        "*Hint: What function have we used in the ML-pipeline to test, or **predict** test data?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O8hxXm66--ox"
      },
      "outputs": [],
      "source": [
        "test_vect = vectorizer.transform(test_tweets)\n",
        "### YOUR CODE HERE: Have your model predict on the third tweet!\n",
        "result = model.predict(test_vect)\n",
        "print('Actual Category: {}\\nPredicted Category: {}'.format(label3, result[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__6MSOHNUy_2"
      },
      "source": [
        "Yay! It predicted it correctly! However, that might not always be the case, as the training set here is so small."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9eYK4QDT9Pg"
      },
      "source": [
        "**Exercise**: Can you trick the logistic regression? Try and make a tweet get classified as \"Water\" when it is about \"Food\".  *Hint: What are some words that were in the Food tweet in the training set that had no significance to food?*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KJnVDK-oUAQV"
      },
      "outputs": [],
      "source": [
        "#@title Trick the logistic regression { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "test_tweet = \"\" #@param {type:\"string\"}\n",
        "true_label = \"Water\" #@param [\"Food\", \"Water\"]\n",
        "\n",
        "test_vect = vectorizer.transform([test_tweet])\n",
        "result = model.predict(test_vect)\n",
        "print('Actual Category: {}\\nPredicted Category: {}'.format(true_label, result[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDaw9DovKNpK"
      },
      "source": [
        "## Logistic Regression for Tweet Classification (Coding Exercise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6Fs9EU6FSvdF"
      },
      "outputs": [],
      "source": [
        "#Split the Data into Training and Testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweet_set, tweet_labels, test_size=0.2, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amJz-5wbKP1J"
      },
      "source": [
        "Let us now build our own regression model for all the training data. Complete the code for `train_model` and `predict` below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z65-9oJQ7mpZ"
      },
      "outputs": [],
      "source": [
        "def train_model(tweets_to_train,train_labels):\n",
        "  \"\"\"\n",
        "  param: tweets_to_train - list of tweets to train on\n",
        "  return: the vectorizer, the logistic regression model, the train_vector\n",
        "  \"\"\"\n",
        "\n",
        "  train_tweets = [\" \".join(t) for t in tweets_to_train]\n",
        "  train_tweets_label = [l for l in train_labels]\n",
        "\n",
        "  ### Your code starts here ###\n",
        "  vectorizer = CountVectorizer() # initialize CountVectorizer\n",
        "  train_vect = vectorizer.fit_transform(train_tweets)\n",
        "\n",
        "  model = LogisticRegression() # create LogisticRegression Model\n",
        "  model.fit(train_vect, train_tweets_label) # train on transformed train data and our labels\n",
        "\n",
        "  ### Your code ends here ###\n",
        "\n",
        "  return model, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z9s_a32jxXXQ"
      },
      "outputs": [],
      "source": [
        "def predict(tweets_to_test, vectorizer, model):\n",
        "  \"\"\"\n",
        "  param: tweets_to_test - list of tweets to test the model on\n",
        "  param: vectorizer - the CountVectorizer\n",
        "  param: model - the LogisticRegression model\n",
        "  return result (the prediction), the test_vect\n",
        "  \"\"\"\n",
        "\n",
        "  test_tweets = [\" \".join(t) for t in  tweets_to_test]\n",
        "  print(test_tweets)\n",
        "\n",
        "  ### Your code starts here\n",
        "\n",
        "  test_vect = vectorizer.transform(test_tweets) # Use .transform to vectorize our tweets\n",
        "  result = model.predict(test_vect) # Have your model predict on the vectorized tweets\n",
        "\n",
        "  ### Your code ends here\n",
        "\n",
        "  return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKdXAnew7B4j"
      },
      "source": [
        "Check out our work in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PeSN-6b1STF7"
      },
      "outputs": [],
      "source": [
        "model, train_countvect = train_model(X_train, y_train)\n",
        "\n",
        "#Predict labels for test set\n",
        "y_pred = predict (X_test, train_countvect, model)\n",
        "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOWekFQkXPjP"
      },
      "source": [
        "# Evaluation\n",
        "Let's see how our classifier did!  We will train our classifier on 80% of the dataset and then test it on 20%. This is called a *train-test split* and is usually done to evaluate models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XadKkUt39krv"
      },
      "outputs": [],
      "source": [
        "table=pd.DataFrame([[\" \".join(t) for t in X_test],y_pred, y_test]).transpose()\n",
        "table.columns = ['Tweet', 'Predicted Category', 'True Category']\n",
        "print(\"Percent Correct: %.2f\" % (sum(table['Predicted Category'] == table['True Category'])/len(table['True Category'])))\n",
        "table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaPh1b8-ZCuk"
      },
      "source": [
        "**Discussion Exercise**: Which categories does the regressor perform best on?  Would the classifier perform better or worse if we only used the food vs water tweets?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvwjKTj9j58o"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Let us look at some stats about the prediction to understand what the model predicted! Review day 1 for a refresher on accuracy-related metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "0AWBAFspnklt"
      },
      "outputs": [],
      "source": [
        "#@title Run this to load the helper function for plotting our confusion matrix!\n",
        "'''\n",
        "Plots the confusion Matrix and saves it\n",
        "'''\n",
        "def plot_confusion_matrix(y_true,y_predicted):\n",
        "  cm = metrics.confusion_matrix(y_true, y_predicted)\n",
        "  print (\"Plotting the Confusion Matrix\")\n",
        "  labels = ['Energy', 'Food', 'Medical', 'None', 'Water']\n",
        "  df_cm = pd.DataFrame(cm,index =labels,columns = labels)\n",
        "  fig = plt.figure()\n",
        "  res = sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n",
        "  plt.yticks([0.5,1.5,2.5,3.5,4.5], labels,va='center')\n",
        "  plt.title('Confusion Matrix - TestData')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sAqXjDIcBvnq"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjNNguK_-esv"
      },
      "source": [
        "Review day 1 for a refresher on accuracy metrics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tRLVrhvdB7Xj"
      },
      "outputs": [],
      "source": [
        "print('The total number of correct predictions are: {}'.format(sum(table['Predicted Category'] == table['True Category'])))\n",
        "print('The total number of incorrect predictions are: {}'.format(sum(table['Predicted Category'] != table['True Category'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LW0GoJB1_wbK"
      },
      "outputs": [],
      "source": [
        "print('Accuracy on the test data is: {:.2f}%'.format(metrics.accuracy_score(y_test, y_pred)*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gg1B7NfkfzA"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1m0JuOtkhmi"
      },
      "source": [
        "**Discuss:** How could you use what you have built to help during a disaster?\n",
        "\n",
        "\n",
        "That is it for today! Next, we'll cover another model - GloVe. Review the concepts we covered today and enjoy the rest of your day!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}